{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP MODELISATION DU CHURN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire\n",
    "\n",
    "1. [Import de la table préparée et Séparation Train / Test](#sect1)\n",
    "2. [Standardisation des variables](#sect2)\n",
    "3. [Modélisation](#sectmodel)\n",
    "  - [Régression pénalisée](#sect41)\n",
    "  - [Random Forest](#sect43)\n",
    "  - [GBM (Gradient Boosting Machine)](#sect44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score,roc_auc_score,roc_curve,auc\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Chargement des données préparées <a name=\"sect1\" ></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repertoire = \"../data\"\n",
    "os.chdir(repertoire)\n",
    "file=open(\"churn_prepared.pydata\",\"rb\")\n",
    "data=pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= data.drop('Churn?',axis=1)\n",
    "y=data['Churn?'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split Apprentissage Test\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.sum(), y_test.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.mean(), y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Standardisation des variables <a name=\"sect2\" ></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "norm=StandardScaler().fit(X_train) \n",
    "X_train = pd.DataFrame(norm.transform(X_train), columns=X.columns)\n",
    "X_test = pd.DataFrame(norm.transform(X_test), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modélisation <a name=\"sectmodel\" ></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de la fonction de lift à utiliser\n",
    "os.chdir('../modules/')\n",
    "from fonctions_metrics import lift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 PARAMETRER LE MODELE REG PENALISEE <a name=\"sect41\" ></a>\n",
    "Choisir ridge (L2) ou lasso (L1) ou elasticnet, avec la classe LogisticRegression\n",
    "Définir une grille d'hyper param pour C\n",
    "et utliser GridSearchCV pour la balayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regression logistique avec pénalité lasso et grid search\n",
    "#### on cherche par CV le meilleur C (1/alpha) le coef de regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [{\"C\":[0.001,0.005,0.01,0.025,0.05,0.075,0.1], \"class_weight\":[\"balanced\",None]}]\n",
    "# Instancie le modele LASSO : construction classe\n",
    "lr=LogisticRegression(penalty='l1',solver='liblinear')\n",
    "# Grid Search : on indique le modele et la grille de param\n",
    "modeleLassoCV = GridSearchCV (lr,param,cv = 4,n_jobs=-1,scoring='roc_auc', refit=True)\n",
    "modeleLassoCV = modeleLassoCV.fit(X_train,y_train)\n",
    "modeleLassoCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeleLassoCV.best_params_['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(modeleLassoCV.cv_results_).sort_values(['rank_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appliquer la méthode fit() : apprend sur les données de TRAIN\n",
    "modeleLassoCV.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des coefficients estimées pour chaque variable\n",
    "coef=list(modeleLassoCV.best_estimator_.coef_[0])\n",
    "coef_df = pd.DataFrame({'Coefficients': list(coef)}, list(X_train.columns.values))\n",
    "coef_df.sort_values(['Coefficients'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combien d'élemnts non nuls ?\n",
    "feature_0= [i for i in coef if i==0] #print(feature_0)\n",
    "print(len(feature_0))\n",
    "# PLus on baisse C, plus de colonnes s'annulent (0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyser les résultats et les performances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction des probabilités de 1 avec \n",
    "# Colonne 1 signifie la seconde modalité du vecteur soit la proba Churn=1\n",
    "probas_test = modeleLassoCV.predict_proba(X_test)[:,1]\n",
    "probas_train = modeleLassoCV.predict_proba(X_train)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score: accuracy taux de bien classé global au cutoff de 0.5\n",
    "print(modeleLassoCV.score(X_test, y_test))\n",
    "print(modeleLassoCV.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC\n",
    "print(roc_auc_score(y_train,probas_train))\n",
    "print(roc_auc_score(y_test,probas_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lift(probas_train,X_train,y_train))\n",
    "print(lift(probas_test,X_test,y_test))\n",
    "\n",
    "# Le lift à 10% : je prend les 10% plus fortes proba de churn (individus les plus risqués),\n",
    "# taux de churn / taux de churn moyen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute lift at 5%\n",
    "print(lift(probas_train,X_train,y_train,p=5))\n",
    "print(lift(probas_test,X_test,y_test,p=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENR LES RESULTATS DANS UN DICTIONNAIRE POUR POUVOIR COMPARER\n",
    "model='Score Lasso'\n",
    "# métriques (liste de dictionnaires)\n",
    "metriques = [{'model':model,'AUC_test':round(roc_auc_score(y_test,probas_test),3),'lift at 5':lift(probas_test,X_test,y_test,p=5),'lift at 10':lift(probas_test,X_test,y_test,p=10)}]\n",
    "metriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat de confusion à un cutoff donné\n",
    "target_names = ['Fidèles','Churners']\n",
    "\n",
    "# métriques au cutoff donné\n",
    "print(classification_report(y_train,modeleLassoCV.predict(X_train), target_names=target_names))\n",
    "# métriques au cutoff par défaut\n",
    "print(classification_report(y_test,modeleLassoCV.predict(X_test), target_names=target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonctions_metrics import CAP_table, auc_et_roc\n",
    "CAP_table(pd.Series(probas_test, index= y_test.index),y_test, stepsize=5,n=10 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_et_roc(y_train, probas_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_et_roc(y_test, probas_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sauvegarde du modèle en pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/my_churn_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(modeleLassoCV, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de réapplication\n",
    "with open(\"../data/my_churn_model.pkl\", \"rb\") as file:\n",
    "    test_reapp_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_reapp_model.predict_proba(X_test.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 PARAMETRER LE MODELE RANDOMFOREST <a name=\"sect43\" ></a>\n",
    "Tester un modle light peu profond et un modele complexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemple randomforest avec des rbres peu profonds\n",
    "rf_light = RandomForestClassifier(                                                                       \n",
    "                                       n_jobs = -1, # coeurs\n",
    "                                       random_state = 42\n",
    "                                     )    \n",
    "\n",
    "param = [{\"n_estimators\":[100,200,300], \"class_weight\":[\"balanced\",None], \"max_depth\":[3,4,5],\"min_samples_split\":[0.05,0.1]}]\n",
    "\n",
    "# Grid Search : on indique le modele et la grille de param\n",
    "modelerfCV = GridSearchCV (rf_light,param,cv = 4,n_jobs=-1,scoring='roc_auc', refit=True)\n",
    "modelerfCV = modelerfCV.fit(X_train,y_train)\n",
    "modelerfCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(modelerfCV.cv_results_).sort_values(['rank_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des variables Light\n",
    "df=pd.DataFrame(modelerfCV.best_estimator_.feature_importances_,X_train.columns.values)\n",
    "df.columns=['Importance']\n",
    "df.sort_values(by='Importance',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyser les résultats et les performances (light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction des probabilités de 1 ( array2d ) avec fonction predict_proba\n",
    "probas_test = modelerfCV.predict_proba(X_test)[:,1]\n",
    "probas_train = modelerfCV.predict_proba(X_train)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC sur train et test avec roc_auc_score\n",
    "print(roc_auc_score(y_train,probas_train))\n",
    "print(roc_auc_score(y_test,probas_test))\n",
    "# un peu de surapprentissage mais peu ce qui est normal car ce sont des arbres peu profonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lift(probas_train,X_train,y_train))\n",
    "print(lift(probas_test,X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train,modelerfCV.predict(X_train), target_names=target_names))\n",
    "print(classification_report(y_test,modelerfCV.predict(X_test), target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AJOUTE LES RESULTATS\n",
    "model='rf'\n",
    "# métriques (liste de dictionnaires)\n",
    "metriques.append([{'model':model,'AUC_test':round(roc_auc_score(y_test,probas_test),3),'lift at 5':lift(probas_test,X_test,y_test,p=5),'lift at 10':lift(probas_test,X_test,y_test,p=10)}])\n",
    "metriques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 PARAMETRER LE MODELE GRADIENT BOOSTING MACHINE <a name=\"sect44\" ></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_noRand05=GradientBoostingClassifier(\n",
    "                                       random_state=42)\n",
    "\n",
    "param = [{\"n_estimators\":[100,200,300], \"max_depth\":[2,3],\"subsample\":[0.9,1.0], \"learning_rate\":[0.001,0.01,0.05]}]\n",
    "\n",
    "# Grid Search : on indique le modele et la grille de param\n",
    "modelegbmCV = GridSearchCV (gbt_noRand05,param,cv = 4,n_jobs=-1,scoring='roc_auc', refit=True)\n",
    "modelegbmCV = modelegbmCV.fit(X_train,y_train)\n",
    "modelegbmCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(modelegbmCV.cv_results_).sort_values(['rank_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse du nb iterations optimal en affichant la fonction de perte sur le test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "niter = modelegbmCV.best_estimator_.n_estimators\n",
    "iter = np.arange(niter) + 1\n",
    "test_deviance = np.zeros((niter,), dtype=np.float64)\n",
    "\n",
    "# staged_predict_proba donne les probabilités à chaque itération\n",
    "for i, y_pred_proba in enumerate(modelegbmCV.best_estimator_.staged_predict_proba(X_test)):\n",
    "    test_deviance[i] = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(iter, test_deviance, label='Test', color='darkorange')\n",
    "plt.plot(iter, modelegbmCV.best_estimator_.train_score_, label='Apprentissage', color='navy')\n",
    "plt.legend(loc=\"upper right\", fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algo avc les mêmes params sauf itértionq qu'on met à 100\n",
    "gbt_Rand=GradientBoostingClassifier(**modelegbmCV.best_params_)\n",
    "gbt_Rand.n_estimators = 100\n",
    "# Apprentissage du modele\n",
    "gbt_Rand.fit(X_train,y_train)\n",
    "gbt_Rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter=gbt_Rand.n_estimators\n",
    "iter = np.arange(niter) + 1\n",
    "test_deviance = np.zeros((niter,), dtype=np.float64)\n",
    "# staged_decision_functio : décision fonction à chaque iteration\n",
    "for i, y_pred_proba in enumerate(gbt_Rand.staged_predict_proba(X_test)):\n",
    "    test_deviance[i] = log_loss(y_test, y_pred_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Erreur sur le test (evolution deviance)\n",
    "plt.plot(iter,test_deviance,label='Test',color='darkorange')\n",
    "        # min vers 100 \n",
    "# Erreur sur apprentissage (evolution deviance)\n",
    "plt.plot(iter,gbt_Rand.train_score_,label='Apprentissage',color='navy')    \n",
    "# Diminution de l'erreur rapport modele precedant (par rapport au oob)\n",
    "#plt.plot(iter,gbt_noRand05.oob_improvement_)\n",
    "plt.legend(loc=\"upper right\", fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyser les résultats et les performances (light) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction des probabilités de 1 , array2d\n",
    "probas_test = modelegbmCV.predict_proba(X_test)[:,1]\n",
    "probas_train = modelegbmCV.predict_proba(X_train)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc_score(y_train,probas_train))\n",
    "print(roc_auc_score(y_test,probas_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train,modelegbmCV.predict(X_train), target_names=target_names))\n",
    "print(classification_report(y_test,modelegbmCV.predict(X_test), target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AJOUTE LES RESULTATS\n",
    "model='gbm heavy'\n",
    "# métriques (liste de dictionnaires)\n",
    "metriques.append([{'model':model,'AUC_test':round(roc_auc_score(y_test,probas_test),3),'lift at 5':lift(probas_test,X_test,y_test,p=5),'lift at 10':lift(probas_test,X_test,y_test,p=10)}])\n",
    "metriques\n",
    "# conclusion  le modele RF est plus predictif mais moins confiance dans sa capacité à etre robuste à moyen terme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction des probabilités de 1 , array2d\n",
    "probas_test = gbt_Rand.predict_proba(X_test)[:,1]\n",
    "probas_train = gbt_Rand.predict_proba(X_train)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc_score(y_train,probas_train))\n",
    "print(roc_auc_score(y_test,probas_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_train,gbt_Rand.predict(X_train), target_names=target_names))\n",
    "print(classification_report(y_test,gbt_Rand.predict(X_test), target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AJOUTE LES RESULTATS\n",
    "model='gbm light'\n",
    "# métriques (liste de dictionnaires)\n",
    "metriques.append([{'model':model,'AUC_test':round(roc_auc_score(y_test,probas_test),3),'lift at 5':lift(probas_test,X_test,y_test,p=5),'lift at 10':lift(probas_test,X_test,y_test,p=10)}])\n",
    "metriques\n",
    "# conclusion  le modele RF est plus predictif mais moins confiance dans sa capacité à etre robuste à moyen terme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Comparaison et conclusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m119"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
