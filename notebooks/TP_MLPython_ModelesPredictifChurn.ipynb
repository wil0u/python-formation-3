{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP MODELISATION DU CHURN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rechargement des données préparées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "repertoire = \"C:\\\\Users\\\\dgr\\\\OneDrive - Publicis Groupe\\\\Documents\\\\Formations\\\\SCU Python\\\\NEW\\\\data\"\n",
    "os.chdir(repertoire)\n",
    "import pickle\n",
    "file=open(\"churn_prepared.pydata\",\"rb\")\n",
    "Xy=pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charger les librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:\\\\Users\\\\dgr\\\\OneDrive - Publicis Groupe\\\\Documents\\\\Formations\\\\SCU Python\\\\NEW\\\\modules\")\n",
    "sys.path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from fonctions_metrics import lift\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,f1_score,roc_auc_score,roc_curve,auc\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETRER LE MODELE REG PENALISEE\n",
    "Choisir ridge (L2) ou lasso (L1) ou elasticnet, avec la classe LogisticRegression\n",
    "Définir une grille d'hyper param pour C\n",
    "et utliser GridSearchCV pour la balayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regression logistique avec pénalité lasso et grid search\n",
    "#### on cherche par CV le meilleur C (1/alpha) le coef de regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = [{\"C\":(....)}]\n",
    "modeleLassoCV = GridSearchCV (....,n_jobs=-1,scoring='roc_auc')\n",
    "\n",
    "modeleLassoCV = modeleLassoCV.fit(X_train,y_train)\n",
    "\n",
    "modeleLassoCV.best_params_\n",
    "#{'C': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# design du modele et app\n",
    "model_LogitL1 =LogisticRegression( ... ).fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# récupérer les coef\n",
    "coef=list(model_LogitL1.coef_[0]) # c un array2d, \n",
    "len(coef) # il y a 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combien d'élemnts non nuls ?\n",
    "feature_0= list(map(lambda x: x==0.0, coef)) #print(feature_0)\n",
    "feature_0.count(True) # 6\n",
    "del feature_0\n",
    "\n",
    "# PLus on baisse C, plus de colonnes s'annulent (0)\n",
    "# print(pd.DataFrame({'Coefficients': list(reg.coef_)}, list(X.columns.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction des probabilités de 1 avec    model_LogitL1.predict_proba(X)\n",
    "probas_test = ...\n",
    "probas_train = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score: accuracy taux de bien classé global au cutoff de 0.5\n",
    "model_LogitL1.score(X_test, y_test)\n",
    "model_LogitL1.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC\n",
    "roc_auc_score(y_train,probas_train)\n",
    "roc_auc_score(y_test,probas_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fonctions_metrics import lift\n",
    "#compute lift at 10%#\n",
    "lift(probas_train,X_train,y_train)\n",
    "lift(probas_test,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute lift at 5%\n",
    "lift(probas_train,X_train,y_train,p=5)\n",
    "lift(probas_test,X_test,y_test,p=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENR LES RESULTATS DANS UN DICTIONNAIRE POUR POUVOIR COMPARER\n",
    "model='Score Lasso'\n",
    "# métriques (liste de dictionnaires)\n",
    "metriques = [{'model':model,'AUC_test':round(roc_auc_score(y_test,probas_test),2),'lift at 5':lift(probas_test,X_test,y_test,p=5),'lift at 10':lift(probas_test,X_test,y_test,p=10)}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mat de confusion à un cutoff donné\n",
    "pred = (probas_test > 0.14).astype(int)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "# parmi les vrais 1 82% sont bien prédits\n",
    "print(pd.crosstab(y_test,pred).apply(lambda r: r/r.sum(), axis=1))\n",
    "# parmi les predits 1 34% sont des vrais positifs\n",
    "print(pd.crosstab(y_test,pred).apply(lambda r: r/r.sum(), axis=0))\n",
    "\n",
    "target_names = ['Fidèles','Churners']\n",
    "\n",
    "# métriques au cutoff donné\n",
    "print(classification_report(y_test,pred, target_names=target_names))\n",
    "# métriques au cutoff par défaut\n",
    "print(classification_report(y_test,model_LogitL1.predict(X_test), target_names=target_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETRER LE MODELE REG PENALISEE ELASTICNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Régression logistique elastic net : SGDClassifier : loss = log --> logistique\n",
    "#### penalty = \"elasticnet\"\n",
    "#### alpha entre 1 et 1000 , et l1_ratio entre 0 et 1\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#Defaults to ‘hinge’, which gives a linear SVM. The ‘log’ loss gives logistic regression, a probabilistic classifier. \n",
    "#l1_ratio : float\n",
    "#The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1. Defaults to 0.15\n",
    "\n",
    "param = [{\"alpha\":(0.001,0.01,0.025,0.05,0.1,0.2)} ]\n",
    "sgdElasticAUCnow = GridSearchCV (SGDClassifier(loss=\"log\", penalty=\"elasticnet\",l1_ratio=0.25,class_weight=None), param, cv = 4,n_jobs=4,scoring='roc_auc')\n",
    "\n",
    "cvelasticnet = sgdElasticAUCnow.fit(X_train,y_train)\n",
    "cvelasticnet.best_params_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticnet = SGDClassifier(loss=\"log\", penalty=\"elasticnet\", alpha=0.05,l1_ratio=0.25,class_weight=None)\n",
    "# Apprentissage du modele\n",
    "elasticnet.fit(X_train,y_train)\n",
    "elasticnet.coef_\n",
    "  #compute lift at 10%#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyser les résultats et les performances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETRER LE MODELE RANDOMFOREST\n",
    "Tester un modle light peu profond et un modele complexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exemple randomforest\n",
    "rf_light = RandomForestClassifier(n_estimators = \n",
    "                            criterion = \"gini\",\n",
    "                                       max_features = ,\n",
    "                                       max_depth = ,\n",
    "                                       min_samples_leaf = ,                                       \n",
    "                                       bootstrap = True,\n",
    "                                       n_jobs = -1, # coeurs\n",
    "                                       random_state = 1234,\n",
    "                                       class_weight=\"balanced\"\n",
    "                                     )    \n",
    "# Apprentissage du modele avec methode fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apprentissage du modele avec methode fit\n",
    "rf_light.fit(X_train,y_train)\n",
    "rf_complex.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSER LES RESULTATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importance des variables\n",
    "df=pd.DataFrame(rf.feature_importances_,X.columns.values)\n",
    "df.columns=['Importance']\n",
    "df.sort_values(by='Importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction des probabilités de 1 ( array2d ) avec fonction predict_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC sur train et test avec roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute lift at 10%#\n",
    "from fonctions_metrics import lift\n",
    "lift(probas_train,X_train,y_train)\n",
    "lift(probas_test,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#compute lift at 5%\n",
    "lift(probas_test,X_test,y_test,p=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# métriques (liste de dictionnaires)\n",
    "metriques = metriques+[{'model':'rf','AUC_test':round(roc_auc_score(y_test,probas_test),2),'lift at 5':lift(probas_test,X_test,y_test,p=5),'lift at 10':lift(probas_test,X_test,y_test,p=10)}]\n",
    "\n",
    "# conclusion  le modele RF est plus predictif mais moins confiance dans sa capacité à etre robuste à moyen terme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARAMETRER LE MODELE BOOSTING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### sans randomisation (subsample=1.0 = on prend tt l'échantillon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_noRand05=GradientBoostingClassifier(loss='deviance', learning_rate=0.05,\n",
    "                           n_estimators=500,\n",
    "                           subsample=1.0, min_samples_split=20, min_samples_leaf=10,\n",
    "                           max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apprentissage du modele\n",
    "gbt_noRand05.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# étude du nb iterations optimal en affichant la fonction de perte sur lech test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter=500\n",
    "iter = np.arange(niter) + 1\n",
    "test_deviance = np.zeros((niter,), dtype=np.float64)\n",
    "# staged_decision_functio : décision fonction à chaque iteration\n",
    "for i, y_pred in enumerate(gbt_noRand05.staged_decision_function(X_test)):\n",
    "    # clf.loss_ assumes that y_test[i] in {0, 1}\n",
    "    test_deviance[i] = gbt_noRand05.loss_(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Erreur sur le test (evolution deviance)\n",
    "plt.plot(iter,test_deviance,label='Test',color='darkorange')\n",
    "        # min vers 100 \n",
    "# Erreur sur apprentissage (evolution deviance)\n",
    "plt.plot(iter,gbt_noRand05.train_score_,label='Apprentissage',color='navy')    \n",
    "# Diminution de l'erreur rapport modele precedant (par rapport au oob)\n",
    "#plt.plot(iter,gbt_noRand05.oob_improvement_)\n",
    "plt.legend(loc=\"upper right\", fontsize=12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction des probabilités de 1 , array2d\n",
    "probas_test = gbt_noRand05.predict_proba(X_test)[:,1]\n",
    "probas_train = gbt_noRand05.predict_proba(X_train)[:,1]\n",
    "  #AUC\n",
    "roc_auc_score(y_train,probas_train)\n",
    "# 1\n",
    "roc_auc_score(y_test,probas_test)\n",
    "# AUC 0.91 sur test\n",
    "\n",
    "#compute lift at 10%#\n",
    "lift(probas_train,X_train,y_train)\n",
    "lift(probas_test,X_test,y_test)\n",
    "# x 7.6\n",
    "#compute lift at 5%\n",
    "lift(probas_test,X_test,y_test,p=5)\n",
    "# 10.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### algo avec randomisation subsample=0.5 et max_feature = racine(nb variable)\n",
    "###########\n",
    "gbt_Rand=GradientBoostingClassifier(loss='deviance', learning_rate=0.05,\n",
    "                           n_estimators=500,\n",
    "                           subsample=0.5, min_samples_split=30, min_samples_leaf=20,\n",
    "                           min_weight_fraction_leaf=0.005,\n",
    "                           max_depth=3,max_leaf_nodes=12,max_features=\"sqrt\")\n",
    "# Apprentissage du modele\n",
    "gbt_Rand.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "niter=500\n",
    "iter = np.arange(niter) + 1\n",
    "test_deviance = np.zeros((niter,), dtype=np.float64)\n",
    "# staged_decision_functio : décision fonction à chaque iteration\n",
    "for i, y_pred in enumerate(gbt_Rand.staged_decision_function(X_test)):\n",
    "    # clf.loss_ assumes that y_test[i] in {0, 1}\n",
    "    test_deviance[i] = gbt_Rand.loss_(y_test, y_pred)\n",
    "\n",
    "# negative cumulative sum of oob improvements (améliration de l'erreur)\n",
    "#out-of-bag (OOB) estimates can be a useful heuristic to estimate the “optimal” number of boosting iterations. OOB estimates are almost identical to cross-validation estimates but they can be computed on-the-fly without the need for repeated model fitting. OOB estimates are only available for Stochastic Gradient Boosting (i.e. subsample < 1.0), the estimates are derived from the improvement in loss based on the examples not included in the bootstrap sample (the so-called out-of-bag examples). The OOB estimator is a pessimistic estimator of the true test loss, but remains a fairly good approximation for a small number of trees.\n",
    "cumsum = -np.cumsum(gbt_Rand.oob_improvement_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(iter,test_deviance,label='Test',color='darkorange')\n",
    "plt.plot(iter,gbt_Rand.train_score_,label='Apprentissage',color='navy')    \n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(iter,cumsum,label='OOB loss')\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.plot(iter,gbt_Rand.oob_improvement_,label='amélioration loss sur OOB')\n",
    "        # min vers 100 \n",
    "# Erreur sur apprentissage (evolution deviance)\n",
    "plt.plot(iter,gbt_noRand05.train_score_,label='Apprentissage',color='navy')    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       # min vers 100\n",
    "     #   pred_at_eachstage=enumerate(gbt_Rand.staged_predict(X_testCR),100)\n",
    "gbt_Rand=GradientBoostingClassifier(loss='deviance', learning_rate=0.05,\n",
    "                           n_estimators=150,\n",
    "                           subsample=0.5, min_samples_split=30, min_samples_leaf=20,\n",
    "                           min_weight_fraction_leaf=0.005,\n",
    "                           max_depth=3,max_leaf_nodes=12,max_features=\"sqrt\")\n",
    "# Apprentissage du modele\n",
    "gbt_Rand.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "probas_test = gbt_Rand.predict_proba(X_test)[:,1]\n",
    "probas_train = gbt_Rand.predict_proba(X_train)[:,1]\n",
    "  #AUC\n",
    "roc_auc_score(y_train,probas_train)\n",
    "# 0,979\n",
    "roc_auc_score(y_test,probas_test)\n",
    "# AUC 0.89 sur test\n",
    "\n",
    "#compute lift at 10%#\n",
    "lift(probas_train,X_train,y_train)\n",
    "lift(probas_test,X_test,y_test)\n",
    "# x 7.3\n",
    "#compute lift at 5%\n",
    "lift(probas_test,X_test,y_test,p=5)\n",
    "# 9.1"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
