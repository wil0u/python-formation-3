{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:23.993097Z",
     "start_time": "2018-08-25T03:56:23.138209Z"
    }
   },
   "outputs": [],
   "source": [
    "# d'après https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:24.006246Z",
     "start_time": "2018-08-25T03:56:23.998204Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use(['ggplot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTIE 1 / Données Fictives de type linéaire pour démontrer comment fonctionne l'apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Génération de données:\n",
    "Nous générons un jeu de données qui suit l'équation y = a * x + b\n",
    "    ordonnée à l'origine, b =4 \n",
    "    pente de la droite, a =3\n",
    "    ajout de bruit gaussien afin de générer un nuage de points autour de la droite\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:24.027559Z",
     "start_time": "2018-08-25T03:56:24.016052Z"
    }
   },
   "outputs": [],
   "source": [
    "X = 2 * np.random.rand(100,1)\n",
    "y = 4 +3 * X+np.random.randn(100,1)\n",
    "\n",
    "#y = a.x + b (a= 4, b=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Représentons le nuage , l'ordonnée à l'origine est bien 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:24.780299Z",
     "start_time": "2018-08-25T03:56:24.032204Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(X,y,'b.')\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "_ =plt.axis([0,2,0,15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Ce nuage de points suit une relation linéaire\n",
    "## Il existe une résolution analytique (calcul direct) : ce modèle s'appelle la régression linéaire (simple ou multiple)\n",
    "Il s'agit d'un modèle qui à partir des données (X, y) détermine a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:24.792752Z",
     "start_time": "2018-08-25T03:56:24.783380Z"
    }
   },
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100,1)),X]\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "print(theta_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>On est proche de nos \"vrais\" coefficients 4 and 3. C'est normal qu'on ne trouve pas les valeurs théoriques car on a ajotué du bruit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appliquons la régression avec les valeurs estimées\n",
    "Prenons deux observations\n",
    "Observation X1 = 0, on attend une prediction = beta_best[0] + beta_best[1]*0\n",
    "Observation X2 = 2, on attend une prediction = beta_best[0] + beta_best[1]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:24.805370Z",
     "start_time": "2018-08-25T03:56:24.795517Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_new = np.array([[0],[2]])\n",
    "X_new_b = np.c_[np.ones((2,1)),X_new]\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Représentons le nuage avec la droite de régression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:25.168502Z",
     "start_time": "2018-08-25T03:56:24.808124Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X_new,y_predict,'r-')\n",
    "plt.plot(X,y,'b.')\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0,2,0,15])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Pour expliquer brièvement la descente de gradient par une analogie :\n",
    " \n",
    " imaginez que vous êtes sur une montagne et que vous êtes aveuglé et que votre tâche est de descendre de la montagne vers la terre plate sans assistance. La seule assistance que vous avez est une app qui vous indique l'altitude par rapport au niveau de la mer.\n",
    " Quelle serait votre approche? Vous commenceriez à descendre dans une direction aléatoire et demander au gadget quelle est l'altitude maintenant. Si le gadget vous indique une altitude plus élevée que votre altitude initiale, alors vous savez que vous avez commencé dans la mauvaise direction. Vous changez de direction et répétez le processus. De cette manière, après de nombreuses itérations, vous descendez enfin avec succès.\n",
    "\n",
    "Voici une analogie pour aider à comprendre comment la descente de gradient fonctionne pour minimiser la fonction de coût dans l'apprentissage automatique.\n",
    "\n",
    "Taille des pas dans n'importe quelle direction = Taux d'apprentissage (Learning rate)\n",
    "L'app vous indique l'altitude = Fonction de coût (Loss Function)\n",
    "La direction de vos pas = Gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction de couts & Gradients\n",
    "\n",
    "<h5> Ci dessous les équations pour calculer la fonction de cout et les gradients pour le modèle de régression linéaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<b>Cost</b>\n",
    "\\begin{equation}\n",
    "J(\\theta) = 1/2m \\sum_{i=1}^{m} (h(\\theta)^{(i)} - y^{(i)})^2 \n",
    "\\end{equation}\n",
    "\n",
    "<b>Gradient</b>\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = 1/m\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_j^{(i)}\n",
    "\\end{equation}\n",
    "\n",
    "<b>Gradients</b>\n",
    "\\begin{equation}\n",
    "\\theta_0: = \\theta_0 -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_0^{(i)})\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\theta_1: = \\theta_1 -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_1^{(i)})\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\theta_2: = \\theta_2 -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_2^{(i)})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_j: = \\theta_j -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_0^{(i)})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:25.180630Z",
     "start_time": "2018-08-25T03:56:25.172904Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def  cal_cost(theta,X,y):\n",
    "    '''\n",
    "    \n",
    "   Calculer fonction de couts\n",
    "    Exemple sur un vecteur à une dimension X\n",
    "    theta = Vector of thetas (coefficients de la régression linéaire)\n",
    "    X     = Row of X's np.zeros((2,j))\n",
    "    y     = Actual y's np.zeros((2,1))\n",
    "    \n",
    "    where:\n",
    "        j is the no of features\n",
    "    '''\n",
    "    \n",
    "    m = len(y)\n",
    "    \n",
    "    predictions = X.dot(theta)\n",
    "    cost = (1/2*m) * np.sum(np.square(predictions-y))\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:25.204130Z",
     "start_time": "2018-08-25T03:56:25.188272Z"
    }
   },
   "outputs": [],
   "source": [
    "# fonction de descente de gradient, ajout d'un parametre pour afficher par iter\n",
    "def gradient_descent(X,y,theta,learning_rate=0.01,iterations=100,printiter=True):\n",
    "    '''\n",
    "    X    = Matrix of X with added bias units\n",
    "    y    = Vector of Y\n",
    "    theta=Vector of thetas np.random.randn(j,1)\n",
    "    learning_rate \n",
    "    iterations = no of iterations\n",
    "    \n",
    "    Returns the final theta vector and array of cost history over no of iterations\n",
    "    '''\n",
    "    m = len(y)\n",
    "    # vecteur contenant autant d'éléments que d'itérations avec la fonction de cout qui décroit\n",
    "    cost_history = np.zeros(iterations)\n",
    "    # vecteur 2D qui contient l'évolution des deux parametres \n",
    "    theta_history = np.zeros((iterations,2))\n",
    "    \n",
    "    # itérations\n",
    "    for it in range(iterations):\n",
    "        # on calcule la prédiction selon le vecteur de coef\n",
    "        prediction = np.dot(X,theta)\n",
    "        \n",
    "        # actualisation des coef en fonction du learning rate et du gradient du param\n",
    "        theta = theta -(1/m)*learning_rate*( X.T.dot((prediction - y)))\n",
    "        # mise à jour de l'historique des coef\n",
    "        theta_history[it,:] =theta.T\n",
    "        # mise à jour de la fonction de cout\n",
    "        cost_history[it]  = cal_cost(theta,X,y)\n",
    "        if printiter==True:\n",
    "            # on affiche itération, coef à deux dec et la fonction de cout\n",
    "            if it<=5:\n",
    "                print('itération :{}, vecteur des coef :{:0.2f}-{:0.2f}, fonction de perte: {:0.1f}'.format(it,theta[0][0],theta[1][0],cal_cost(theta,X,y)))\n",
    "            else:\n",
    "                if it%25==0:\n",
    "                    print('dérivé du coef de x',(1/m)*learning_rate*( X.T.dot((prediction - y)))[1][0])\n",
    "                    print('itération :{}, vecteur des coef :{:0.2f}-{:0.2f}, fonction de perte: {:0.1f}'.format(it,theta[0][0],theta[1][0],cal_cost(theta,X,y)))\n",
    "                else:\n",
    "                    pass\n",
    "        \n",
    "    return theta, cost_history, theta_history\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Réalisons une descente de gradient avec 1000 iterations et learning rate 0.01. \n",
    "Le vecteur de coef. démarre avec un tirage aléatoire (gaussien)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:27.126960Z",
     "start_time": "2018-08-25T03:56:27.062811Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr =0.1\n",
    "n_iter = 501\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "# matrice X avec une constante\n",
    "X_b = np.c_[np.ones((len(X),1)),X]\n",
    "theta,cost_history,theta_history = gradient_descent(X_b,y,theta,lr,n_iter)\n",
    "\n",
    "\n",
    "print('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\n",
    "print('Final cost/MSE:  {:0.3f}'.format(cost_history[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Let's plot the cost history over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T04:01:31.912400Z",
     "start_time": "2018-08-25T04:01:31.604459Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "ax.set_ylabel('J(Theta)')\n",
    "ax.set_xlabel('Iterations')\n",
    "_=ax.plot(range(n_iter),cost_history,'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Apres 150 iterations, le cout devient \"flat\" : les itérations suivantes apportent plus rien. Concentrons nous sur les 200 premières itérations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:22:30.561842Z",
     "start_time": "2018-08-19T05:22:30.371532Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fig,ax = plt.subplots(figsize=(10,8))\n",
    "_=ax.plot(range(200),cost_history[:200],'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Les couts descendent très vite dans les premières itérations puis plafonne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorons différentes combinaisons : itérations x learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction pour représenter la descente de gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T04:06:54.948007Z",
     "start_time": "2018-08-25T04:06:54.916435Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_GD(n_iter,lr,ax,ax1=None):\n",
    "     \"\"\"\n",
    "     n_iter = no of iterations\n",
    "     lr = Learning Rate\n",
    "     ax = Axis to plot the Gradient Descent\n",
    "     ax1 = Axis to plot cost_history vs Iterations plot\n",
    "\n",
    "     \"\"\"\n",
    "     _ = ax.plot(X,y,'b.')\n",
    "     theta = np.random.randn(2,1)\n",
    "\n",
    "     tr =0.1\n",
    "     cost_history = np.zeros(n_iter)\n",
    "     for i in range(n_iter):\n",
    "        pred_prev = X_b.dot(theta)\n",
    "        theta,h,_ = gradient_descent(X_b,y,theta,lr,1)\n",
    "        pred = X_b.dot(theta)\n",
    "\n",
    "        cost_history[i] = h[0]\n",
    "\n",
    "        if ((i % 25 == 0) ):\n",
    "            _ = ax.plot(X,pred,'r-',alpha=tr)\n",
    "            if tr < 0.8:\n",
    "                tr = tr+0.2\n",
    "     if not ax1== None:\n",
    "        _ = ax1.plot(range(n_iter),cost_history,'b.')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Représentation selon différentes combinaisons : itérations x learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T04:07:09.977910Z",
     "start_time": "2018-08-25T04:07:00.484794Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,25),dpi=200)\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "it_lr =[(2000,0.001),(500,0.01),(200,0.05),(100,0.1)]\n",
    "count =0\n",
    "for n_iter, lr in it_lr:\n",
    "    count += 1\n",
    "    \n",
    "    ax = fig.add_subplot(4, 2, count)\n",
    "    count += 1\n",
    "   \n",
    "    ax1 = fig.add_subplot(4,2,count)\n",
    "    \n",
    "    ax.set_title(\"lr:{}\".format(lr))\n",
    "    ax1.set_title(\"Iterations:{}\".format(n_iter))\n",
    "    plot_GD(n_iter,lr,ax,ax1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Les lignes rouges montrent comment la descente de gradient progresse lentement vers les valeurs finales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-17T09:38:15.809824Z",
     "start_time": "2018-08-17T09:38:15.807296Z"
    }
   },
   "source": [
    "## Représentations par cas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:14.735491Z",
     "start_time": "2018-08-19T05:23:14.487772Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_,ax = plt.subplots(figsize=(14,10))\n",
    "plot_GD(100,0.1,ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:17.900469Z",
     "start_time": "2018-08-19T05:23:17.882614Z"
    }
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X,y,theta,learning_rate=0.01,iterations=10):\n",
    "    '''\n",
    "    X    = Matrix of X with added bias units\n",
    "    y    = Vector of Y\n",
    "    theta=Vector of thetas np.random.randn(j,1)\n",
    "    learning_rate \n",
    "    iterations = no of iterations\n",
    "    \n",
    "    Returns the final theta vector and array of cost history over no of iterations\n",
    "    '''\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    \n",
    "    \n",
    "    for it in range(iterations):\n",
    "        cost =0.0\n",
    "        # on actualise par individu i et on passe tous les individus\n",
    "        for i in range(m):\n",
    "            rand_ind = np.random.randint(0,m)\n",
    "            X_i = X[rand_ind,:].reshape(1,X.shape[1])\n",
    "            y_i = y[rand_ind].reshape(1,1)\n",
    "            prediction = np.dot(X_i,theta)\n",
    "\n",
    "            theta = theta -(1/m)*learning_rate*( X_i.T.dot((prediction - y_i)))\n",
    "            cost += cal_cost(theta,X_i,y_i)\n",
    "        cost_history[it]  = cost\n",
    "        \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:20.091678Z",
     "start_time": "2018-08-19T05:23:19.910817Z"
    }
   },
   "outputs": [],
   "source": [
    "lr =0.5\n",
    "n_iter = 50\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "X_b = np.c_[np.ones((len(X),1)),X]\n",
    "theta,cost_history = stochastic_gradient_descent(X_b,y,theta,lr,n_iter)\n",
    "\n",
    "\n",
    "print('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\n",
    "print('Final cost/MSE:  {:0.3f}'.format(cost_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:20.736824Z",
     "start_time": "2018-08-19T05:23:20.484570Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "ax.set_ylabel('{J(Theta)}',rotation=0)\n",
    "ax.set_xlabel('{Iterations}')\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "_=ax.plot(range(n_iter),cost_history,'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:25.240288Z",
     "start_time": "2018-08-19T05:23:25.217657Z"
    }
   },
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent(X,y,theta,learning_rate=0.01,iterations=10,batch_size =20):\n",
    "    '''\n",
    "    X    = Matrix of X without added bias units\n",
    "    y    = Vector of Y\n",
    "    theta=Vector of thetas np.random.randn(j,1)\n",
    "    learning_rate \n",
    "    iterations = no of iterations\n",
    "    \n",
    "    Returns the final theta vector and array of cost history over no of iterations\n",
    "    '''\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    n_batches = int(m/batch_size)\n",
    "    \n",
    "    for it in range(iterations):\n",
    "        cost =0.0\n",
    "        indices = np.random.permutation(m)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "         # on actualise par i paquet d'individu (batch size) et tout le paquet est utilisé pour actualiser l'estimation\n",
    "        for i in range(0,m,batch_size):\n",
    "            X_i = X[i:i+batch_size]\n",
    "            y_i = y[i:i+batch_size]\n",
    "            \n",
    "            X_i = np.c_[np.ones(len(X_i)),X_i]\n",
    "           \n",
    "            prediction = np.dot(X_i,theta)\n",
    "\n",
    "            theta = theta -(1/m)*learning_rate*( X_i.T.dot((prediction - y_i)))\n",
    "            cost += cal_cost(theta,X_i,y_i)\n",
    "        cost_history[it]  = cost\n",
    "        \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:25.835249Z",
     "start_time": "2018-08-19T05:23:25.724421Z"
    }
   },
   "outputs": [],
   "source": [
    "lr =0.1\n",
    "n_iter = 200\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "\n",
    "theta,cost_history = minibatch_gradient_descent(X,y,theta,lr,n_iter)\n",
    "\n",
    "\n",
    "print('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\n",
    "print('Final cost/MSE:  {:0.3f}'.format(cost_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:26.512988Z",
     "start_time": "2018-08-19T05:23:26.326207Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "ax.set_ylabel('{J(Theta)}',rotation=0)\n",
    "ax.set_xlabel('{Iterations}')\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "_=ax.plot(range(n_iter),cost_history,'b.')"
   ]
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Gradient Descent-Python",
    "public": true
   },
   "id": ""
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.233px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "461.183px",
    "left": "846.167px",
    "right": "138.333px",
    "top": "127px",
    "width": "559.667px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
