{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédire les habitudes de consommation d'alcool des adolescents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sommaire : \n",
    "\n",
    "1. [Import et sélection des variables](#sect1)\n",
    "2. [Transformation des variables et séparation Train / Test](#sect2)\n",
    "3. [Modélisation](#sect3)\n",
    "4. [Performance des modèles](#sect4)\n",
    "5. [Amélioration des hyperparamètres (Grid Search)](#sect5)\n",
    "6. [Autre modèles et algorithmes](#sect6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import et sélection des vraiables<a name=\"sect1\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student = pd.read_csv(\"https://raw.githubusercontent.com/udacity/machine-learning/master/projects/student_intervention/student-data.csv\", sep=\",\").dropna()\n",
    "student.rename(columns={'sex':'gender'}, inplace=True)\n",
    "student['alcohol_index'] = (5*student['Dalc'] + 2*student['Walc'])/7\n",
    "# Niveau de consommation d'alcool (création d'une cible\n",
    "student['acl'] = student['alcohol_index'] <= 2\n",
    "student['acl'] = student['acl'].map({True: 'Low', False: 'High'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sélection des variables / features que l'on souhaite conserver pour la prédiction\n",
    "features = ['gender', 'age', 'address', 'famsize', 'Pstatus', 'Medu',\n",
    "       'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime',\n",
    "       'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery',\n",
    "       'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout','health' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student[features].head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptif des colonnes : \n",
    "\n",
    "* **ADRESS** : student's home address type (binary: 'U' - urban or 'R' - rural) \n",
    "* **FAMSIZE** : Family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)\n",
    "* **PSTATUS** Parent's cohabitation status (binary: 'T' - living together or 'A' - living apart)\n",
    "* **MEDU** (resp. **FEDU**) : Mother's (resp. Father) education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education, or 4 - higher education)\n",
    "* **MJOB** (resp. **FJOB**): Mother's (resp. Father) job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')\n",
    "* **REASON** : Reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')\n",
    "* **GUARDIAN** : Student's guardian (nominal: 'mother', 'father' or 'other')\n",
    "* **TRAVELTIME** : Home to school travel time (numeric: 1 - &lt;15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - &gt;1 hour)\n",
    "* **STUDYTIME** : Weekly study time (numeric: 1 - &lt;2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - &gt;10 hours)\n",
    "* **FAILURES** :           Number of past class failures (numeric: n if 1&lt;=n&lt;3, else 4)\n",
    "* **SCHOOLSUP** : Extra educational support (binary: yes or no)\n",
    "* **FAMSUP** : Family educational support (binary: yes or no)\n",
    "* **PAID** : Extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)\n",
    "* **ACTIVITIES** : Extra-curricular activities (binary: yes or no) \n",
    "* **NURSERY** :  Attended nursery school (binary: yes or no)\n",
    "* **HIGHER** : Wants to take higher education (binary: yes or no)\n",
    "* **INTERNET** :  Internet access at home (binary: yes or no)\n",
    "* **ROMANTIC** : With a romantic relationship (binary: yes or no)\n",
    "* **FAMREL** : Quality of family relationships (numeric: from 1 - very bad to 5 - excellent)\n",
    "* **FREETIME** : free time after school (numeric: from 1 - very low to 5 - very high)\n",
    "* **GOOUT** : Going out with friends (numeric: from 1 - very low to 5 - very high)\n",
    "* **HEALTH** : Current health status (numeric: from 1 - very bad to 5 - very good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse descriptive rapide : \n",
    "student[features].describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distinction des variables qualitative / quantitative et identification de la target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative :\n",
    "featuresquali = ['gender','famsize','address','Pstatus','Mjob','Fjob','reason','guardian','schoolsup', 'famsup', 'paid', 'activities', 'nursery',\n",
    "       'higher', 'internet', 'romantic']\n",
    "\n",
    "# Quantitative :\n",
    "featuresquanti = ['age', 'Medu','Fedu',  'traveltime', 'studytime',\n",
    "       'failures',  'famrel', 'freetime', 'goout','health']\n",
    "\n",
    "# Target\n",
    "target = 'acl' # consommation d'alcool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student[featuresquali].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student = student[features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformation des variables et séparation Train / Test <a name=\"sect2\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avant de séparer le Train et le test on \"dichotomise\" les variables Quali\n",
    "studentPrep = pd.get_dummies(student[features], columns=featuresquali, drop_first=True) # l'option drop first permet de supprimer une modalité comme référence (Utile dans le cas de la régression logistique)\n",
    "studentPrep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On transforme la variable cible en numérique afin que Scikit-learn puisse se situer dans un problème de classification binaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student['acl'] = student['acl'].map({'Low':0, 'High':1}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des variables explicatives et de la variable Target\n",
    "X = studentPrep\n",
    "y = student[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X),len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse de la variable cible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taux de cible des adolescent ayant une consommation élevée d'alcool \n",
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition train /  test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application de la fonction avec tirage de 30% en test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y, test_size=0.3, random_state=42) # stratify permet de conserver la même répartition de la cible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonne pratique : feature scaling afin de normaliser les données "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "les paramètres de standardisation sont appris sur l'échantillon d'apprentissage et réappliquer sur l'échantillon de test afin de conserver les mêmes transformation de données lors de l'apprentissage du test et de la réapplication sur de nouvelles données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation d'on objet de normalisation \n",
    "# Normalisation : moyenne nulle et variance unitaire\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[featuresquanti].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrainement et application de la normalisation sur les données de train \n",
    "scaler.fit(X_train[featuresquanti])\n",
    "X_train[featuresquanti] = scaler.transform(X_train[featuresquanti])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[featuresquanti].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[featuresquanti].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réapplication sur les données de test\n",
    "X_test[featuresquanti] = scaler.transform(X_test[featuresquanti])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[featuresquanti].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modélisation <a name=\"sect3\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle de base de la régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de l'objet classifier\n",
    "classifier_lr = LogisticRegression()#class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apprentissage\n",
    "classifier_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des coefficients estimées pour chaque variable\n",
    "coef=list(classifier_lr.coef_[0])\n",
    "coef_df = pd.DataFrame({'Coefficients': list(coef)}, list(X_train.columns.values))\n",
    "coef_df.sort_values(['Coefficients'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prédiction sur le Train et le Test\n",
    "probas_train = classifier_lr.predict_proba(X_train) # pour les probas\n",
    "probas_test = classifier_lr.predict_proba(X_test) # pour les probas\n",
    "\n",
    "predict_train = classifier_lr.predict(X_train) # pour les prédictions avec cutoff = 0.5\n",
    "predict_test = classifier_lr.predict(X_test)  # pour les prédictions avec cutoff = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les Probabilités sont celles de 0 et 1 et sont complémentaires\n",
    "print(classifier_lr.classes_)\n",
    "probas_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération uniquement de la proba d'interêt\n",
    "probas_train = probas_train[:,1]\n",
    "probas_test = probas_test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation sur un exemple : \n",
    "probas_test[0], y_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification de la proba moyenne en fonction du paramètre class_weight=\"balanced\"\n",
    "probas_train.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Métriques de performance <a name=\"sect4\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On va utiliser des métriques pour évaluer le modèle\n",
    "certaines métriques dépendent d'un curseur sur la proba Y=1 (threshold dependant), d'autres au contraire sont comme la logloss une quantité (threshold invariant), c'est le cas de l'AUC qui plus elle est proche de 1 meilleur sera le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importation de métriques\n",
    "from sklearn.metrics import classification_report,accuracy_score,f1_score,roc_auc_score,recall_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "pd.crosstab(y_train, predict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrice plus Visuel : \n",
    "# Récupération de la fonction dans les modules\n",
    "import os\n",
    "os.chdir('../')\n",
    "from modules.fonctions_metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(y_train, predict_train, classes=[\"low risk\",\"high risk\"], \n",
    "                      title='Matrice de confusion échantillon Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compraisons des métriques entre le train et le test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Echantillon Train \\n ---------\")\n",
    "print(classification_report(y_train, predict_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Echantillon Test \\n ---------\")\n",
    "print(classification_report(y_test, predict_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tracons les courbes de ROC (compte tenu du peu de volumétrie la forme des courbes n'est pas lisse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.fonctions_metrics import auc_et_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_et_roc(y_train,probas_train)\n",
    "auc_et_roc(y_test,probas_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tracons les courbes de Gain et Lift (compte tenu du peu de volumétrie la forme des courbes n'est pas lisse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.fonctions_metrics import CAP_table, lift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_lift = CAP_t.able(pd.Series(probas_train,index=y_train.index), y_train,stepsize = 1, n=100)\n",
    "table_lift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(table_lift['% positifs cumulés sur le total des positifs'], label=\"Mon Modèle\")\n",
    "plt.plot([0, 100], [0, 100], c = 'r', linestyle = '--', label = 'Modèle aléatoire')\n",
    "plt.plot([0,  np.round(y_train.sum() / len(y_train)*100,0), 100], [0, 100, 100], c = 'g', linestyle = '-', label = 'Modèle parfait')\n",
    "plt.legend()\n",
    "plt.xlabel('% de scorés')\n",
    "plt.ylabel('% de scorés à raison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(table_lift['Lift'])\n",
    "plt.xlabel('% de scorés')\n",
    "plt.ylabel('Lift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Amélioration des hyperparamètres (Grid Search) <a name=\"sect5\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle de régression logistique pénalisée\n",
    "\n",
    "Une régression pénalisée de type ridge (L2) permet de contraindre l'espace des coef estimés pour ne pas qu'ils prennent des valeurs contradictoires et très élevées,\n",
    "Si la régression est de type lasso (L1) alors certains coefficients vont être annulés.\n",
    "Le paramètre C contrôle cela : \n",
    "C = Inverse of regularization strength; must be a positive float = smaller values specify stronger regularization.\n",
    "\n",
    "**A noter que dans certaines classes, le paramètre est 1/C**\n",
    "\n",
    "*NB : Dans Sklearn le coefficient C est égale à l'inverse du poids de régularization*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit la liste des hyper paramètres de l'on souhaite tester\n",
    "param = [{ \"C\": [0.01,0.025,0.05,0.1],\"class_weight\": [\"balanced\", None] }]\n",
    "\n",
    "# On initialise un objet classifier LR sur lequel nous allons tester toutes les itérations possibles\n",
    "clf_lr=LogisticRegression(penalty='l1', solver=\"liblinear\")\n",
    "\n",
    "# On initialise un objet gridsearch grâce à la liste d'arguments et au classifier \n",
    "# on lui donne une métrique à maximiser pour identifier le meilleur modèle\n",
    "# on spécifie sur combien d'échantillon de cross validation se fera le calcul des métriques (cv)  \n",
    "modelCV= GridSearchCV(clf_lr, param, cv = 4, n_jobs = -1, scoring=\"recall\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On lance l'entraînement de tous les modèles\n",
    "modelCV = modelCV.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Résultats détaillés :\n",
    "pd.DataFrame(modelCV.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quel sont les meilleurs paramètres ? \n",
    "modelCV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meilleur estimator\n",
    "modelCV.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des coefficients estimées pour chaque variable\n",
    "coef=list(modelCV.best_estimator_.coef_[0])\n",
    "coef_df = pd.DataFrame({'Coefficients': list(coef)}, list(X_train.columns.values))\n",
    "\n",
    "coef_df.sort_values(['Coefficients'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L'argument refit=True permet de réentrainer directement le meilleur estimator à la fin de l'entrainement du Grid Search\n",
    "# on a donc pas besoin de ré-entraîner un modèle mais on peut directement utiliser l'objet modelCV\n",
    "# Dans le cas où le best_estimator_ ne nous convient pas il faudrait entraîner un nouveau modèle avec les paramètres voulus\n",
    "\n",
    "predict_train = modelCV.predict(X_train)\n",
    "predict_test = modelCV.predict(X_test)\n",
    "probas_train = modelCV.predict_proba(X_train)[:,1]\n",
    "probas_test = modelCV.predict_proba(X_test)[:,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation des performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Echantillon Train \\n ---------\")\n",
    "print(classification_report(y_train, predict_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Echantillon Test \\n ---------\")\n",
    "print(classification_report(y_test, predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_et_roc(y_train,probas_train)\n",
    "auc_et_roc(y_test,probas_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Autre modèles et algorithmes <a name=\"sect6\" ></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbre CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "modele_arbre=DecisionTreeClassifier(random_state = 42, max_depth = 3, min_samples_leaf = 30)\n",
    "modele_arbre.fit(X_train, y_train)\n",
    "predict_train = modele_arbre.predict(X_train)\n",
    "predict_test = modele_arbre.predict(X_test)\n",
    "probas_train = modele_arbre.predict_proba(X_train)[:,1]\n",
    "probas_test = modele_arbre.predict_proba(X_test)[:,1]\n",
    "print(\"Echantillon Train \\n ---------\")\n",
    "print(classification_report(y_train, predict_train))\n",
    "\n",
    "print(\"Echantillon Test \\n ---------\")\n",
    "print(classification_report(y_test, predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modele_arbre=DecisionTreeClassifier(random_state = 42, max_depth = 3, min_samples_leaf = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modele_arbre.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_train = modele_arbre.predict(X_train)\n",
    "predict_test = modele_arbre.predict(X_test)\n",
    "probas_train = modele_arbre.predict_proba(X_train)[:,1]\n",
    "probas_test = modele_arbre.predict_proba(X_test)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Echantillon Train \\n ---------\")\n",
    "print(classification_report(y_train, predict_train))\n",
    "\n",
    "print(\"Echantillon Test \\n ---------\")\n",
    "print(classification_report(y_test, predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "tree.plot_tree(modele_arbre,class_names=True, max_depth=4,proportion=True, fontsize=10,filled=True,feature_names=X_train.columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators = 300, max_depth = 3,min_samples_leaf = 30, random_state = 42, class_weight=\"balanced\" )    \n",
    "rf.fit(X_train, y_train)\n",
    "predict_train = rf.predict(X_train)\n",
    "predict_test = rf.predict(X_test)\n",
    "probas_train = rf.predict_proba(X_train)[:,1]\n",
    "probas_test = rf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Echantillon Train \\n ---------\")\n",
    "print(classification_report(y_train, predict_train))\n",
    "\n",
    "print(\"Echantillon Test \\n ---------\")\n",
    "print(classification_report(y_test, predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([X_train.columns,rf.feature_importances_]).T.sort_values([1],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_et_roc(y_train,probas_train)\n",
    "auc_et_roc(y_test,probas_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM (Gradient Boosting Machine) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbm =GradientBoostingClassifier(learning_rate=0.01,\n",
    "                           n_estimators=400, max_depth=2, random_state=42,subsample=0.9 )\n",
    "gbm.fit(X_train, y_train)\n",
    "predict_train = gbm.predict(X_train)\n",
    "predict_test = gbm.predict(X_test)\n",
    "probas_train = gbm.predict_proba(X_train)[:,1]\n",
    "probas_test = gbm.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Echantillon Train \\n ---------\")\n",
    "print(classification_report(y_train, predict_train))\n",
    "\n",
    "print(\"Echantillon Test \\n ---------\")\n",
    "print(classification_report(y_test, predict_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([X_train.columns,gbm.feature_importances_]).T.sort_values([1],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/:m119"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "name": "Python_Classification_exemple",
  "notebookId": 1320569712343186
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
